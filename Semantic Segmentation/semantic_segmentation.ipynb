{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports + Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Seed for Reproducibility\n",
    "torch.manual_seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc.\n",
    "LEARNING_RATE = 1e-2\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "#BATCH_SIZE = 8\n",
    "#NUM_EPOCHS = 100\n",
    "NUM_WORKERS = 32\n",
    "IMAGE_HEIGHT = 160 # 1280 originally\n",
    "IMAGE_WIDTH = 240  # 1918 originally\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "TRAIN_IMG_DIR = \"data/train_images/\"\n",
    "TRAIN_MASK_DIR = \"data/train_masks/\"\n",
    "VAL_IMG_DIR = \"data/val_images/\"\n",
    "VAL_MASK_DIR = \"data/val_masks/\"\n",
    "TEST_IMG_DIR = \"data/test_images/\"\n",
    "TEST_MASK_DIR = \"data/test_masks/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directories\n",
    "data_dir = 'data'\n",
    "images_dir = 'images'\n",
    "masks_dir = 'masks'\n",
    "train_images_dir = os.path.join(data_dir, 'train_images')\n",
    "train_masks_dir = os.path.join(data_dir, 'train_masks')\n",
    "test_images_dir = os.path.join(data_dir, 'test_images')\n",
    "test_masks_dir = os.path.join(data_dir, 'test_masks')\n",
    "val_images_dir = os.path.join(data_dir, 'val_images')\n",
    "val_masks_dir = os.path.join(data_dir, 'val_masks')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(train_images_dir, exist_ok=True)\n",
    "os.makedirs(train_masks_dir, exist_ok=True)\n",
    "os.makedirs(test_images_dir, exist_ok=True)\n",
    "os.makedirs(test_masks_dir, exist_ok=True)\n",
    "os.makedirs(val_images_dir, exist_ok=True)\n",
    "os.makedirs(val_masks_dir, exist_ok=True)\n",
    "\n",
    "# Get the list of all image files\n",
    "image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
    "\n",
    "# Shuffle the list to ensure randomness\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Calculate the number of images for training, validation and testing\n",
    "total_images = len(image_files)\n",
    "train_count = int(total_images * 0.7)\n",
    "val_count = int(total_images * 0.15)\n",
    "test_count = total_images - train_count - val_count\n",
    "\n",
    "# Move the files\n",
    "for i, image_file in enumerate(image_files):\n",
    "    mask_file = image_file.replace('.jpg', '_mask.gif')\n",
    "    \n",
    "    if i < train_count:\n",
    "        # Move to training directories\n",
    "        shutil.copy(os.path.join(images_dir, image_file), os.path.join(train_images_dir, image_file))\n",
    "        shutil.copy(os.path.join(masks_dir, mask_file), os.path.join(train_masks_dir, mask_file))\n",
    "    elif i < train_count + val_count:\n",
    "        # Move to validation directories\n",
    "        shutil.copy(os.path.join(images_dir, image_file), os.path.join(val_images_dir, image_file))\n",
    "        shutil.copy(os.path.join(masks_dir, mask_file), os.path.join(val_masks_dir, mask_file))\n",
    "    else:\n",
    "        # Move to testing directories\n",
    "        shutil.copy(os.path.join(images_dir, image_file), os.path.join(test_images_dir, image_file))\n",
    "        shutil.copy(os.path.join(masks_dir, mask_file), os.path.join(test_masks_dir, mask_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarvanaDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(\n",
    "    train_dir,\n",
    "    train_maskdir,\n",
    "    val_dir,\n",
    "    val_maskdir,\n",
    "    test_dir,\n",
    "    test_maskdir,\n",
    "    batch_size,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    test_transform,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    train_ds = CarvanaDataset(\n",
    "        image_dir=train_dir,\n",
    "        mask_dir=train_maskdir,\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = CarvanaDataset(\n",
    "        image_dir=val_dir,\n",
    "        mask_dir=val_maskdir,\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    test_ds = CarvanaDataset(\n",
    "        image_dir=test_dir,\n",
    "        mask_dir=test_maskdir,\n",
    "        transform=test_transform,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model, loss_fn, device=\"cuda\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            preds = model(x)\n",
    "            loss = loss_fn(preds, y)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.sigmoid(preds)\n",
    "            # Calculate the loss\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / (\n",
    "                (preds + y).sum() + 1e-8\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n",
    "    )\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
    "    print(\"Loss: \", (total_loss/len(loader)))\n",
    "    return dice_score/len(loader), total_loss/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_as_imgs(loader, model, folder=\"saved_images/\", device=\"cuda\"):\n",
    "    model.eval()\n",
    "    # Check if the directory exists, if not, create it\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    for idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "        torchvision.utils.save_image(preds, f\"{folder}/pred_{idx}.png\")\n",
    "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    x = torch.randn((3, 1, 161, 161))\n",
    "    model = UNET(in_channels=1, out_channels=1)\n",
    "    preds = model(x)\n",
    "    assert preds.shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAG Optimizer\n",
    "class NAG(Optimizer):\n",
    "    def __init__(self, params, lr=required, momentum=0, weight_decay=0):\n",
    "        defaults = dict(lr=lr, lr_old=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        super(NAG, self).__init__(params, defaults)\n",
    "\n",
    "    @property\n",
    "    def supports_memory_efficient_fp16(self):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def supports_flat_params(self):\n",
    "        return True\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group[\"weight_decay\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "            lr = group[\"lr\"]\n",
    "            lr_old = group.get(\"lr_old\", lr)\n",
    "            lr_correct = lr / lr_old if lr_old > 0 else lr\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                p_data_fp32 = p.data\n",
    "                if p_data_fp32.dtype in {torch.float16, torch.bfloat16}:\n",
    "                    p_data_fp32 = p_data_fp32.float()\n",
    "\n",
    "                d_p = p.grad.data.float()\n",
    "                param_state = self.state[p]\n",
    "                if \"momentum_buffer\" not in param_state:\n",
    "                    param_state[\"momentum_buffer\"] = torch.zeros_like(d_p)\n",
    "                else:\n",
    "                    param_state[\"momentum_buffer\"] = param_state[\"momentum_buffer\"].to(\n",
    "                        d_p\n",
    "                    )\n",
    "\n",
    "                buf = param_state[\"momentum_buffer\"]\n",
    "\n",
    "                if weight_decay != 0:\n",
    "                    p_data_fp32.mul_(1 - lr * weight_decay)\n",
    "                p_data_fp32.add_(buf, alpha=momentum * momentum * lr_correct)\n",
    "                p_data_fp32.add_(d_p, alpha=-(1 + momentum) * lr)\n",
    "\n",
    "                buf.mul_(momentum * lr_correct).add_(d_p, alpha=-lr)\n",
    "\n",
    "                if p.data.dtype in {torch.float16, torch.bfloat16}:\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "\n",
    "            group[\"lr_old\"] = lr\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProxSG\n",
    "class ProxSG(Optimizer):\n",
    "    def __init__(self, params, lr=required, lambda_=required):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "\n",
    "        if lambda_ is not required and lambda_ < 0.0:\n",
    "            raise ValueError(\"Invalid lambda: {}\".format(lambda_))\n",
    "\n",
    "        defaults = dict(lr=lr, lambda_=lambda_)\n",
    "        super(ProxSG, self).__init__(params, defaults)\n",
    "\n",
    "    def calculate_d(self, x, grad_f, lambda_, lr):\n",
    "        '''\n",
    "            Calculate d for Omega(x) = ||x||_1\n",
    "        '''\n",
    "        trial_x = torch.zeros_like(x)\n",
    "        pos_shrink = x - lr * grad_f - lr * \\\n",
    "            lambda_  # new x is larger than lr * lambda_\n",
    "        neg_shrink = x - lr * grad_f + lr * \\\n",
    "            lambda_  # new x is less than -lr * lambda_\n",
    "        pos_shrink_idx = (pos_shrink > 0)\n",
    "        neg_shrink_idx = (neg_shrink < 0)\n",
    "        trial_x[pos_shrink_idx] = pos_shrink[pos_shrink_idx]\n",
    "        trial_x[neg_shrink_idx] = neg_shrink[neg_shrink_idx]\n",
    "        d = trial_x - x\n",
    "\n",
    "        return d\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad_f = p.grad.data\n",
    "\n",
    "                if len(p.shape) > 1:  # weights\n",
    "                    s = self.calculate_d(\n",
    "                        p.data, grad_f, group['lambda_'], group['lr'])\n",
    "                    p.data.add_(s, alpha=1)\n",
    "                else:  # bias\n",
    "                    p.data.add_(grad_f, alpha=-group['lr'])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVRG Optimizer\n",
    "class SVRG(Optimizer):\n",
    "    r\"\"\" implement SVRG \"\"\" \n",
    "\n",
    "    def __init__(self, params, lr=required, freq =10):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "\n",
    "        defaults = dict(lr=lr, freq=freq)\n",
    "        self.counter = 0\n",
    "        self.counter2 = 0\n",
    "        self.flag = False\n",
    "        super(SVRG, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(SVRG, self).__setstate__(state)\n",
    "        # for group in self.param_groups:\n",
    "        #     group.setdefault('m', )\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            freq = group['freq']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                param_state = self.state[p]\n",
    "                \n",
    "                if 'large_batch' not in param_state:\n",
    "                    buf = param_state['large_batch'] = torch.zeros_like(p.data)\n",
    "                    buf.add_(d_p) #add first large, low variance batch\n",
    "                    #need to add the second term in the step equation; the gradient for the original step!\n",
    "                    buf2 = param_state['small_batch'] = torch.zeros_like(p.data)\n",
    "\n",
    "                buf = param_state['large_batch']\n",
    "                buf2 = param_state['small_batch']\n",
    "\n",
    "                if self.counter == freq:\n",
    "                    buf.data = d_p.clone() #copy new large batch. Begining of new inner loop\n",
    "                    temp = torch.zeros_like(p.data)\n",
    "                    buf2.data = temp.clone()\n",
    "                    \n",
    "                if self.counter2 == 1:\n",
    "                    buf2.data.add_(d_p) #first small batch gradient for inner loop!\n",
    "\n",
    "                #dont update parameters when computing large batch (low variance gradients)\n",
    "                if self.counter != freq and self.flag != False:\n",
    "                    p.data.add_((d_p - buf2 + buf), alpha=-group['lr'])\n",
    "\n",
    "        self.flag = True #rough way of not updating the weights the FIRST time we calculate the large batch gradient\n",
    "        \n",
    "        if self.counter == freq:\n",
    "            self.counter = 0\n",
    "            self.counter2 = 0\n",
    "\n",
    "        self.counter += 1    \n",
    "        self.counter2 += 1\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prodigy(Optimizer):\n",
    "    r\"\"\"\n",
    "    Implements Adam with Prodigy step-sizes.\n",
    "    Leave LR set to 1 unless you encounter instability.\n",
    "   \n",
    "    Arguments:\n",
    "        params (iterable):\n",
    "            Iterable of parameters to optimize or dicts defining parameter groups.\n",
    "        lr (float):\n",
    "            Learning rate adjustment parameter. Increases or decreases the Prodigy learning rate.\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        beta3 (float):\n",
    "            coefficients for computing the Prodidy stepsize using running averages.\n",
    "            If set to None, uses the value of square root of beta2 (default: None).\n",
    "        eps (float):\n",
    "            Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-8).\n",
    "        weight_decay (float):\n",
    "            Weight decay, i.e. a L2 penalty (default: 0).\n",
    "        decouple (boolean):\n",
    "            Use AdamW style decoupled weight decay\n",
    "        use_bias_correction (boolean):\n",
    "            Turn on Adam's bias correction. Off by default.\n",
    "        safeguard_warmup (boolean):\n",
    "            Remove lr from the denominator of D estimate to avoid issues during warm-up stage. Off by default.\n",
    "        d0 (float):\n",
    "            Initial D estimate for D-adaptation (default 1e-6). Rarely needs changing.\n",
    "        d_coef (float):\n",
    "            Coefficient in the expression for the estimate of d (default 1.0).\n",
    "            Values such as 0.5 and 2.0 typically work as well. \n",
    "            Changing this parameter is the preferred way to tune the method.\n",
    "        growth_rate (float):\n",
    "            prevent the D estimate from growing faster than this multiplicative rate.\n",
    "            Default is inf, for unrestricted. Values like 1.02 give a kind of learning\n",
    "            rate warmup effect.\n",
    "        fsdp_in_use (bool):\n",
    "            If you're using sharded parameters, this should be set to True. The optimizer\n",
    "            will attempt to auto-detect this, but if you're using an implementation other\n",
    "            than PyTorch's builtin version, the auto-detection won't work.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1.0,\n",
    "                 betas=(0.9, 0.999), beta3=None,\n",
    "                 eps=1e-8, weight_decay=0, decouple=True, \n",
    "                 use_bias_correction=False, safeguard_warmup=False,\n",
    "                 d0=1e-6, d_coef=1.0, growth_rate=float('inf'),\n",
    "                 fsdp_in_use=False):\n",
    "        if not 0.0 < d0:\n",
    "            raise ValueError(\"Invalid d0 value: {}\".format(d0))\n",
    "        if not 0.0 < lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 < eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "\n",
    "        if decouple and weight_decay > 0:\n",
    "            print(f\"Using decoupled weight decay\")\n",
    "\n",
    "       \n",
    "        defaults = dict(lr=lr, betas=betas, beta3=beta3,\n",
    "                        eps=eps, weight_decay=weight_decay,\n",
    "                        d=d0, d0=d0, d_max=d0,\n",
    "                        d_numerator=0.0, d_coef=d_coef,\n",
    "                        k=0, growth_rate=growth_rate,\n",
    "                        use_bias_correction=use_bias_correction,\n",
    "                        decouple=decouple, safeguard_warmup=safeguard_warmup,\n",
    "                        fsdp_in_use=fsdp_in_use)\n",
    "        self.d0 = d0\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @property\n",
    "    def supports_memory_efficient_fp16(self):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def supports_flat_params(self):\n",
    "        return True\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        d_denom = 0.0\n",
    "\n",
    "        group = self.param_groups[0]\n",
    "        use_bias_correction = group['use_bias_correction']\n",
    "        beta1, beta2 = group['betas']\n",
    "        beta3 = group['beta3']\n",
    "        if beta3 is None:\n",
    "            beta3 = math.sqrt(beta2)\n",
    "        k = group['k']\n",
    "\n",
    "        d = group['d']\n",
    "        d_max = group['d_max']\n",
    "        d_coef = group['d_coef']\n",
    "        lr = max(group['lr'] for group in self.param_groups)\n",
    "\n",
    "        if use_bias_correction:\n",
    "            bias_correction = ((1 - beta2**(k+1))**0.5) / (1 - beta1**(k+1))\n",
    "        else:\n",
    "            bias_correction = 1\n",
    "\n",
    "        dlr = d*lr*bias_correction\n",
    "       \n",
    "        growth_rate = group['growth_rate']\n",
    "        decouple = group['decouple']\n",
    "        fsdp_in_use = group['fsdp_in_use']\n",
    "\n",
    "        d_numerator = group['d_numerator']\n",
    "        d_numerator *= beta3\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            decay = group['weight_decay']\n",
    "            k = group['k']\n",
    "            eps = group['eps']\n",
    "            group_lr = group['lr']\n",
    "            d0 = group['d0']\n",
    "            safeguard_warmup = group['safeguard_warmup']\n",
    "\n",
    "            if group_lr not in [lr, 0.0]:\n",
    "                raise RuntimeError(f\"Setting different lr values in different parameter groups is only supported for values of 0\")\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                if hasattr(p, \"_fsdp_flattened\"):\n",
    "                    fsdp_in_use = True\n",
    "               \n",
    "                grad = p.grad.data\n",
    "               \n",
    "                # Apply weight decay (coupled variant)\n",
    "                if decay != 0 and not decouple:\n",
    "                    grad.add_(p.data, alpha=decay)\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if 'step' not in state:\n",
    "                    state['step'] = 0\n",
    "                    state['s'] = torch.zeros_like(p.data).detach()\n",
    "                    state['p0'] = p.detach().clone()\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data).detach()\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data).detach()\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "               \n",
    "                s = state['s']\n",
    "                p0 = state['p0']\n",
    "\n",
    "                if group_lr > 0.0:\n",
    "                    # we use d / d0 instead of just d to avoid getting values that are too small\n",
    "                    d_numerator += (d / d0) * dlr * torch.dot(grad.flatten(), (p0.data - p.data).flatten()).item()\n",
    "\n",
    "                    # Adam EMA updates\n",
    "                    exp_avg.mul_(beta1).add_(grad, alpha=d * (1-beta1))\n",
    "                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=d * d * (1-beta2))\n",
    "\n",
    "                    if safeguard_warmup:\n",
    "                        s.mul_(beta3).add_(grad, alpha=((d / d0) * d))\n",
    "                    else:\n",
    "                        s.mul_(beta3).add_(grad, alpha=((d / d0) * dlr))\n",
    "                    d_denom += s.abs().sum().item()\n",
    "\n",
    "            ######\n",
    "\n",
    "        d_hat = d\n",
    "\n",
    "        # if we have not done any progres, return\n",
    "        # if we have any gradients available, will have d_denom > 0 (unless \\|g\\|=0)\n",
    "        if d_denom == 0:\n",
    "            return loss\n",
    "       \n",
    "        if lr > 0.0:\n",
    "            if fsdp_in_use:\n",
    "                dist_tensor = torch.zeros(2).cuda()\n",
    "                dist_tensor[0] = d_numerator\n",
    "                dist_tensor[1] = d_denom\n",
    "                dist.all_reduce(dist_tensor, op=dist.ReduceOp.SUM)\n",
    "                global_d_numerator = dist_tensor[0]\n",
    "                global_d_denom = dist_tensor[1]\n",
    "            else:\n",
    "                global_d_numerator = d_numerator\n",
    "                global_d_denom = d_denom\n",
    "\n",
    "            d_hat = d_coef * global_d_numerator / global_d_denom\n",
    "            if d == group['d0']:\n",
    "                d = max(d, d_hat)\n",
    "            d_max = max(d_max, d_hat)\n",
    "            d = min(d_max, d * growth_rate)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            group['d_numerator'] = global_d_numerator\n",
    "            group['d_denom'] = global_d_denom\n",
    "            group['d'] = d\n",
    "            group['d_max'] = d_max\n",
    "            group['d_hat'] = d_hat\n",
    "\n",
    "            decay = group['weight_decay']\n",
    "            k = group['k']\n",
    "            eps = group['eps']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(d * eps)\n",
    "\n",
    "                # Apply weight decay (decoupled variant)\n",
    "                if decay != 0 and decouple:\n",
    "                    p.data.add_(p.data, alpha=-decay * dlr)\n",
    "\n",
    "\n",
    "                ### Take step\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-dlr)\n",
    "\n",
    "            group['k'] = k + 1\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ranger(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3,                       # lr\n",
    "                 alpha=0.5, k=6, N_sma_threshhold=5,           # Ranger options\n",
    "                 betas=(.95, 0.999), eps=1e-5, weight_decay=0,  # Adam options\n",
    "                 # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n",
    "                 use_gc=True, gc_conv_only=False\n",
    "                 ):\n",
    "\n",
    "        # parameter checks\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        if not lr > 0:\n",
    "            raise ValueError(f'Invalid Learning Rate: {lr}')\n",
    "        if not eps > 0:\n",
    "            raise ValueError(f'Invalid eps: {eps}')\n",
    "\n",
    "        # parameter comments:\n",
    "        # beta1 (momentum) of .95 seems to work better than .90...\n",
    "        # N_sma_threshold of 5 seems better in testing than 4.\n",
    "        # In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n",
    "\n",
    "        # prep defaults and init torch.optim base\n",
    "        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas,\n",
    "                        N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "        # adjustable threshold\n",
    "        self.N_sma_threshhold = N_sma_threshhold\n",
    "\n",
    "        # look ahead params\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "        # radam buffer for state\n",
    "        self.radam_buffer = [[None, None, None] for ind in range(10)]\n",
    "\n",
    "        # gc on or off\n",
    "        self.use_gc = use_gc\n",
    "\n",
    "        # level of gradient centralization\n",
    "        self.gc_gradient_threshold = 3 if gc_conv_only else 1\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Ranger, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        # Evaluate averages and grad, update param tensors\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        'Ranger optimizer does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]  # get state dict for this param\n",
    "\n",
    "                if len(state) == 0:  # if first time to run...init dictionary with our desired entries\n",
    "                    # if self.first_run_check==0:\n",
    "                    # self.first_run_check=1\n",
    "                    #print(\"Initializing slow buffer...should not see this at load from saved model!\")\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "\n",
    "                    # look ahead weight storage now in state dict\n",
    "                    state['slow_buffer'] = torch.empty_like(p.data)\n",
    "                    state['slow_buffer'].copy_(p.data)\n",
    "\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n",
    "                        p_data_fp32)\n",
    "\n",
    "                # begin computations\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                # GC operation for Conv layers and FC layers\n",
    "                if grad.dim() > self.gc_gradient_threshold:\n",
    "                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # compute variance mov avg\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "                # compute mean moving avg\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                buffered = self.radam_buffer[int(state['step'] % 10)]\n",
    "\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * \\\n",
    "                        state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "                    if N_sma > self.N_sma_threshhold:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (\n",
    "                            N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay']\n",
    "                                     * group['lr'], p_data_fp32)\n",
    "\n",
    "                # apply lr\n",
    "                if N_sma > self.N_sma_threshhold:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size*\n",
    "                                         group['lr'])\n",
    "\n",
    "                else:\n",
    "                    p_data_fp32.add_(exp_avg, alpha=-step_size * group['lr'])\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "                # integrated look ahead...\n",
    "                # we do it at the param level instead of group level\n",
    "                if state['step'] % group['k'] == 0:\n",
    "                    # get access to slow param tensor\n",
    "                    slow_p = state['slow_buffer']\n",
    "                    # (fast weights - slow weights) * alpha\n",
    "                    slow_p.add_(p.data - slow_p, alpha=self.alpha)\n",
    "                    # copy interpolated weights to RAdam param tensor\n",
    "                    p.data.copy_(slow_p)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Eval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.002):\n",
    "\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.min_validation_loss = float('inf')\n",
    "        self.counter = 0\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss + self.min_delta >= self.min_validation_loss:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        print(\"Early Stopping Counter: \", self.counter)\n",
    "        print(\"Lowest Validation Loss: \", self.min_validation_loss)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader, val_loader, model, optimizer, loss_fn, opt_name, scaler):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(15):\n",
    "        loop = tqdm(loader)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (data, targets) in enumerate(loop):\n",
    "            data = data.to(device=DEVICE)\n",
    "            targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "            # forward\n",
    "            with torch.cuda.amp.autocast():\n",
    "                predictions = model(data)\n",
    "                loss = loss_fn(predictions, targets)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # update tqdm loop\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "            # Update the total loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        epoch_losses.append((total_loss/len(loader)))\n",
    "\n",
    "        # Test on validation set\n",
    "        val_accuracy, val_loss = check_accuracy(val_loader, model, loss_fn, device=DEVICE)\n",
    "\n",
    "         # Save the model if it has the best accuracy so far\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), f\"best_{opt_name}.pkl\")\n",
    "\n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Rotate(limit=35, p=1.0),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=5, min_delta=0.002)\n",
    "model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_loader, val_loader, test_loader = get_loaders(\n",
    "    TRAIN_IMG_DIR,\n",
    "    TRAIN_MASK_DIR,\n",
    "    VAL_IMG_DIR,\n",
    "    VAL_MASK_DIR,\n",
    "    TEST_IMG_DIR,\n",
    "    TEST_MASK_DIR,\n",
    "    BATCH_SIZE,\n",
    "    train_transform,\n",
    "    val_transforms,\n",
    "    test_transforms,\n",
    "    NUM_WORKERS,\n",
    "    PIN_MEMORY,\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\"\"\"while True:\n",
    "    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
    "    val_loss = check_accuracy(val_loader, model, loss_fn, device=DEVICE)\n",
    "    \n",
    "    # save model with the highest dice score\n",
    "    if val_loss < early_stopping.min_validation_loss:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\":optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint)\n",
    "    \n",
    "    if early_stopping.early_stop(val_loss):             \n",
    "        break\n",
    "\n",
    "    # print some examples to a folder for testing purposes\n",
    "    save_predictions_as_imgs(\n",
    "        val_loader, model, folder=\"saved_images/\", device=DEVICE\n",
    "    )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [optim.SGD, optim.Adam, optim.RMSprop, optim.Adadelta, Ranger, Prodigy, NAG, ProxSG, SVRG]\n",
    "optimizer_names = ['SGD', 'Adam', 'RMSProp', 'Adadelta', 'Ranger', 'Prodigy', 'NAG', 'ProxSG', 'SVRG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = {}\n",
    "test_accuracy = {}\n",
    "convergence_rate = {}\n",
    "\n",
    "for opt_fn, opt_name in zip(optimizers, optimizer_names):\n",
    "    print(f\"\\nTraining with {opt_name} optimizer:\")\n",
    "    model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
    "    # Adam and RMSProp need lower learning rate or else NAN\n",
    "    if opt_name == 'Adam' or opt_name == 'RMSProp':\n",
    "        optimizer = opt_fn(model.parameters(), lr=1e-3)\n",
    "    elif opt_name == 'ProxSG':\n",
    "        optimizer = opt_fn(model.parameters(), lr=LEARNING_RATE, lambda_=0.0001)\n",
    "    else:\n",
    "        optimizer = opt_fn(model.parameters(), lr=LEARNING_RATE)\n",
    "    loss = train_fn(train_loader, val_loader, model, optimizer, loss_fn, opt_name, scaler)\n",
    "    print(\"---TESTING ON TRAINED MODEL---\")\n",
    "    # Load the best model for testing\n",
    "    model.load_state_dict(torch.load(f\"best_{opt_name}.pkl\"))\n",
    "    accuracy, test_loss = check_accuracy(test_loader, model, loss_fn, device=DEVICE)\n",
    "    conv_rate = [loss[i] - loss[i-1] for i in range(1, len(loss))]\n",
    "\n",
    "    train_loss[opt_name] = loss\n",
    "    test_accuracy[opt_name] = (accuracy.cpu())*100\n",
    "    convergence_rate[opt_name] = conv_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(6, 10))\n",
    "\n",
    "# Plot losses\n",
    "for opt_name in optimizer_names:\n",
    "    losses = train_loss[opt_name]\n",
    "    axs[0].plot(losses, label=f'{opt_name}')\n",
    "axs[0].set_title('Training Losses')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot accuracies\n",
    "for opt_name in optimizer_names:\n",
    "    accuracy = test_accuracy[opt_name]\n",
    "    bar = axs[1].bar(opt_name, accuracy, label=f'{opt_name}')\n",
    "    axs[1].text(bar[0].get_x() + bar[0].get_width() / 2, bar[0].get_height(), f'{accuracy:.2f}%', ha='center', va='bottom')\n",
    "axs[1].set_title('Test Accuracy')\n",
    "axs[1].set_xlabel('Optimizer')\n",
    "axs[1].set_ylabel('Accuracy (%)')\n",
    "axs[1].legend()\n",
    "\n",
    "# Plot convergence rates\n",
    "for opt_name in optimizer_names:\n",
    "    losses = train_loss[opt_name]\n",
    "    convergence_rates = [losses[i] - losses[i-1] for i in range(1, len(losses))]\n",
    "    axs[2].plot(range(1, len(losses)), convergence_rates, label=f'{opt_name}')\n",
    "axs[2].set_title('Convergence Rate')\n",
    "axs[2].set_xlabel('Epoch')\n",
    "axs[2].set_ylabel('Convergence Rate')\n",
    "axs[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
