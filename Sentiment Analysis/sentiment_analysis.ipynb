{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dc0a1d994a2c45b880cb2166f45026d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3353940f1ba74b308a1bcc1b26ef1241",
              "IPY_MODEL_0bd32cccc35a4bf08aa27ccc70e5eb64",
              "IPY_MODEL_3f659fad49f148168d5152bf298026d7"
            ],
            "layout": "IPY_MODEL_73fcce18779744c5bac1865528cd777c"
          }
        },
        "3353940f1ba74b308a1bcc1b26ef1241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c1c33f1df654575a38185328715b501",
            "placeholder": "​",
            "style": "IPY_MODEL_c5d5029bac914ab2b63f72a44f3da7c3",
            "value": "/root/.dgl/sst.zip: 100%"
          }
        },
        "0bd32cccc35a4bf08aa27ccc70e5eb64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48b0d590c44e4d58af5c6bfa0cbce5ea",
            "max": 929945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d09800c24d54291bd584aefce7d1d65",
            "value": 929945
          }
        },
        "3f659fad49f148168d5152bf298026d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1cea837961143ee99970b5c4ec7bf29",
            "placeholder": "​",
            "style": "IPY_MODEL_a02ccb9596284d1583687617e0d8ad2a",
            "value": " 930k/930k [00:00&lt;00:00, 2.04MB/s]"
          }
        },
        "73fcce18779744c5bac1865528cd777c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c1c33f1df654575a38185328715b501": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5d5029bac914ab2b63f72a44f3da7c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48b0d590c44e4d58af5c6bfa0cbce5ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d09800c24d54291bd584aefce7d1d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1cea837961143ee99970b5c4ec7bf29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a02ccb9596284d1583687617e0d8ad2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl"
      ],
      "metadata": {
        "id": "29bct_U2nS6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "XUlSbPqFrd-o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYbV4nzGnNNQ",
        "outputId": "00a4c2af-e451-4998-f9be-5ffd50caaaf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import collections\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import dgl\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as INIT\n",
        "import torch.optim as optim\n",
        "from dgl.data.tree import SSTDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizers"
      ],
      "metadata": {
        "id": "yruUrkbWrfil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NAG Optimizer\n",
        "class NAG(Optimizer):\n",
        "  def __init__(self, params, lr=required, momentum=0, weight_decay=0):\n",
        "      defaults = dict(lr=lr, lr_old=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "      super(NAG, self).__init__(params, defaults)\n",
        "\n",
        "  @property\n",
        "  def supports_memory_efficient_fp16(self):\n",
        "      return True\n",
        "\n",
        "  @property\n",
        "  def supports_flat_params(self):\n",
        "      return True\n",
        "\n",
        "  def step(self, closure=None):\n",
        "      \"\"\"Performs a single optimization step.\n",
        "\n",
        "      Args:\n",
        "          closure (callable, optional): A closure that reevaluates the model\n",
        "              and returns the loss.\n",
        "      \"\"\"\n",
        "      loss = None\n",
        "      if closure is not None:\n",
        "          loss = closure()\n",
        "\n",
        "      for group in self.param_groups:\n",
        "          weight_decay = group[\"weight_decay\"]\n",
        "          momentum = group[\"momentum\"]\n",
        "          lr = group[\"lr\"]\n",
        "          lr_old = group.get(\"lr_old\", lr)\n",
        "          lr_correct = lr / lr_old if lr_old > 0 else lr\n",
        "\n",
        "          for p in group[\"params\"]:\n",
        "              if p.grad is None:\n",
        "                  continue\n",
        "\n",
        "              p_data_fp32 = p.data\n",
        "              if p_data_fp32.dtype in {torch.float16, torch.bfloat16}:\n",
        "                  p_data_fp32 = p_data_fp32.float()\n",
        "\n",
        "              d_p = p.grad.data.float()\n",
        "              param_state = self.state[p]\n",
        "              if \"momentum_buffer\" not in param_state:\n",
        "                  param_state[\"momentum_buffer\"] = torch.zeros_like(d_p)\n",
        "              else:\n",
        "                  param_state[\"momentum_buffer\"] = param_state[\"momentum_buffer\"].to(\n",
        "                      d_p\n",
        "                  )\n",
        "\n",
        "              buf = param_state[\"momentum_buffer\"]\n",
        "\n",
        "              if weight_decay != 0:\n",
        "                  p_data_fp32.mul_(1 - lr * weight_decay)\n",
        "              p_data_fp32.add_(buf, alpha=momentum * momentum * lr_correct)\n",
        "              p_data_fp32.add_(d_p, alpha=-(1 + momentum) * lr)\n",
        "\n",
        "              buf.mul_(momentum * lr_correct).add_(d_p, alpha=-lr)\n",
        "\n",
        "              if p.data.dtype in {torch.float16, torch.bfloat16}:\n",
        "                  p.data.copy_(p_data_fp32)\n",
        "\n",
        "          group[\"lr_old\"] = lr\n",
        "\n",
        "      return loss"
      ],
      "metadata": {
        "id": "lbxNN8aOoKy6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ProxSG\n",
        "class ProxSG(Optimizer):\n",
        "  def __init__(self, params, lr=required, lambda_=required):\n",
        "      if lr is not required and lr < 0.0:\n",
        "          raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "\n",
        "      if lambda_ is not required and lambda_ < 0.0:\n",
        "          raise ValueError(\"Invalid lambda: {}\".format(lambda_))\n",
        "\n",
        "      defaults = dict(lr=lr, lambda_=lambda_)\n",
        "      super(ProxSG, self).__init__(params, defaults)\n",
        "\n",
        "  def calculate_d(self, x, grad_f, lambda_, lr):\n",
        "      '''\n",
        "          Calculate d for Omega(x) = ||x||_1\n",
        "      '''\n",
        "      trial_x = torch.zeros_like(x)\n",
        "      pos_shrink = x - lr * grad_f - lr * \\\n",
        "          lambda_  # new x is larger than lr * lambda_\n",
        "      neg_shrink = x - lr * grad_f + lr * \\\n",
        "          lambda_  # new x is less than -lr * lambda_\n",
        "      pos_shrink_idx = (pos_shrink > 0)\n",
        "      neg_shrink_idx = (neg_shrink < 0)\n",
        "      trial_x[pos_shrink_idx] = pos_shrink[pos_shrink_idx]\n",
        "      trial_x[neg_shrink_idx] = neg_shrink[neg_shrink_idx]\n",
        "      d = trial_x - x\n",
        "\n",
        "      return d\n",
        "\n",
        "  def step(self, closure=None):\n",
        "\n",
        "      loss = None\n",
        "      if closure is not None:\n",
        "          loss = closure()\n",
        "\n",
        "      for group in self.param_groups:\n",
        "\n",
        "          for p in group['params']:\n",
        "              if p.grad is None:\n",
        "                  continue\n",
        "              grad_f = p.grad.data\n",
        "\n",
        "              if len(p.shape) > 1:  # weights\n",
        "                  s = self.calculate_d(\n",
        "                      p.data, grad_f, group['lambda_'], group['lr'])\n",
        "                  p.data.add_(s, alpha=1)\n",
        "              else:  # bias\n",
        "                  p.data.add_(grad_f, alpha=-group['lr'])\n",
        "      return loss"
      ],
      "metadata": {
        "id": "0ULVRhW0oNof"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVRG Optimizer\n",
        "class SVRG(Optimizer):\n",
        "  r\"\"\" implement SVRG \"\"\"\n",
        "\n",
        "  def __init__(self, params, lr=required, freq =10):\n",
        "      if lr is not required and lr < 0.0:\n",
        "          raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "\n",
        "      defaults = dict(lr=lr, freq=freq)\n",
        "      self.counter = 0\n",
        "      self.counter2 = 0\n",
        "      self.flag = False\n",
        "      super(SVRG, self).__init__(params, defaults)\n",
        "\n",
        "  def __setstate__(self, state):\n",
        "      super(SVRG, self).__setstate__(state)\n",
        "      # for group in self.param_groups:\n",
        "      #     group.setdefault('m', )\n",
        "\n",
        "  def step(self, closure=None):\n",
        "      \"\"\"Performs a single optimization step.\n",
        "\n",
        "      Arguments:\n",
        "          closure (callable, optional): A closure that reevaluates the model\n",
        "              and returns the loss.\n",
        "      \"\"\"\n",
        "      loss = None\n",
        "      if closure is not None:\n",
        "          loss = closure()\n",
        "\n",
        "      for group in self.param_groups:\n",
        "          freq = group['freq']\n",
        "          for p in group['params']:\n",
        "              if p.grad is None:\n",
        "                  continue\n",
        "              d_p = p.grad.data\n",
        "              param_state = self.state[p]\n",
        "\n",
        "              if 'large_batch' not in param_state:\n",
        "                  buf = param_state['large_batch'] = torch.zeros_like(p.data)\n",
        "                  buf.add_(d_p) #add first large, low variance batch\n",
        "                  #need to add the second term in the step equation; the gradient for the original step!\n",
        "                  buf2 = param_state['small_batch'] = torch.zeros_like(p.data)\n",
        "\n",
        "              buf = param_state['large_batch']\n",
        "              buf2 = param_state['small_batch']\n",
        "\n",
        "              if self.counter == freq:\n",
        "                  buf.data = d_p.clone() #copy new large batch. Begining of new inner loop\n",
        "                  temp = torch.zeros_like(p.data)\n",
        "                  buf2.data = temp.clone()\n",
        "\n",
        "              if self.counter2 == 1:\n",
        "                  buf2.data.add_(d_p) #first small batch gradient for inner loop!\n",
        "\n",
        "              #dont update parameters when computing large batch (low variance gradients)\n",
        "              if self.counter != freq and self.flag != False:\n",
        "                  p.data.add_((d_p - buf2 + buf), alpha=-group['lr'])\n",
        "\n",
        "      self.flag = True #rough way of not updating the weights the FIRST time we calculate the large batch gradient\n",
        "\n",
        "      if self.counter == freq:\n",
        "          self.counter = 0\n",
        "          self.counter2 = 0\n",
        "\n",
        "      self.counter += 1\n",
        "      self.counter2 += 1\n",
        "\n",
        "      return loss"
      ],
      "metadata": {
        "id": "H9WBgtEZoWTz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Prodigy(Optimizer):\n",
        "  r\"\"\"\n",
        "  Implements Adam with Prodigy step-sizes.\n",
        "  Leave LR set to 1 unless you encounter instability.\n",
        "\n",
        "  Arguments:\n",
        "      params (iterable):\n",
        "          Iterable of parameters to optimize or dicts defining parameter groups.\n",
        "      lr (float):\n",
        "          Learning rate adjustment parameter. Increases or decreases the Prodigy learning rate.\n",
        "      betas (Tuple[float, float], optional): coefficients used for computing\n",
        "          running averages of gradient and its square (default: (0.9, 0.999))\n",
        "      beta3 (float):\n",
        "          coefficients for computing the Prodidy stepsize using running averages.\n",
        "          If set to None, uses the value of square root of beta2 (default: None).\n",
        "      eps (float):\n",
        "          Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-8).\n",
        "      weight_decay (float):\n",
        "          Weight decay, i.e. a L2 penalty (default: 0).\n",
        "      decouple (boolean):\n",
        "          Use AdamW style decoupled weight decay\n",
        "      use_bias_correction (boolean):\n",
        "          Turn on Adam's bias correction. Off by default.\n",
        "      safeguard_warmup (boolean):\n",
        "          Remove lr from the denominator of D estimate to avoid issues during warm-up stage. Off by default.\n",
        "      d0 (float):\n",
        "          Initial D estimate for D-adaptation (default 1e-6). Rarely needs changing.\n",
        "      d_coef (float):\n",
        "          Coefficient in the expression for the estimate of d (default 1.0).\n",
        "          Values such as 0.5 and 2.0 typically work as well.\n",
        "          Changing this parameter is the preferred way to tune the method.\n",
        "      growth_rate (float):\n",
        "          prevent the D estimate from growing faster than this multiplicative rate.\n",
        "          Default is inf, for unrestricted. Values like 1.02 give a kind of learning\n",
        "          rate warmup effect.\n",
        "      fsdp_in_use (bool):\n",
        "          If you're using sharded parameters, this should be set to True. The optimizer\n",
        "          will attempt to auto-detect this, but if you're using an implementation other\n",
        "          than PyTorch's builtin version, the auto-detection won't work.\n",
        "  \"\"\"\n",
        "  def __init__(self, params, lr=1.0,\n",
        "                betas=(0.9, 0.999), beta3=None,\n",
        "                eps=1e-8, weight_decay=0, decouple=True,\n",
        "                use_bias_correction=False, safeguard_warmup=False,\n",
        "                d0=1e-6, d_coef=1.0, growth_rate=float('inf'),\n",
        "                fsdp_in_use=False):\n",
        "      if not 0.0 < d0:\n",
        "          raise ValueError(\"Invalid d0 value: {}\".format(d0))\n",
        "      if not 0.0 < lr:\n",
        "          raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "      if not 0.0 < eps:\n",
        "          raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "      if not 0.0 <= betas[0] < 1.0:\n",
        "          raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "      if not 0.0 <= betas[1] < 1.0:\n",
        "          raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "\n",
        "      if decouple and weight_decay > 0:\n",
        "          print(f\"Using decoupled weight decay\")\n",
        "\n",
        "\n",
        "      defaults = dict(lr=lr, betas=betas, beta3=beta3,\n",
        "                      eps=eps, weight_decay=weight_decay,\n",
        "                      d=d0, d0=d0, d_max=d0,\n",
        "                      d_numerator=0.0, d_coef=d_coef,\n",
        "                      k=0, growth_rate=growth_rate,\n",
        "                      use_bias_correction=use_bias_correction,\n",
        "                      decouple=decouple, safeguard_warmup=safeguard_warmup,\n",
        "                      fsdp_in_use=fsdp_in_use)\n",
        "      self.d0 = d0\n",
        "      super().__init__(params, defaults)\n",
        "\n",
        "  @property\n",
        "  def supports_memory_efficient_fp16(self):\n",
        "      return False\n",
        "\n",
        "  @property\n",
        "  def supports_flat_params(self):\n",
        "      return True\n",
        "\n",
        "  def step(self, closure=None):\n",
        "      \"\"\"Performs a single optimization step.\n",
        "\n",
        "      Arguments:\n",
        "          closure (callable, optional): A closure that reevaluates the model\n",
        "              and returns the loss.\n",
        "      \"\"\"\n",
        "      loss = None\n",
        "      if closure is not None:\n",
        "          loss = closure()\n",
        "\n",
        "      d_denom = 0.0\n",
        "\n",
        "      group = self.param_groups[0]\n",
        "      use_bias_correction = group['use_bias_correction']\n",
        "      beta1, beta2 = group['betas']\n",
        "      beta3 = group['beta3']\n",
        "      if beta3 is None:\n",
        "          beta3 = math.sqrt(beta2)\n",
        "      k = group['k']\n",
        "\n",
        "      d = group['d']\n",
        "      d_max = group['d_max']\n",
        "      d_coef = group['d_coef']\n",
        "      lr = max(group['lr'] for group in self.param_groups)\n",
        "\n",
        "      if use_bias_correction:\n",
        "          bias_correction = ((1 - beta2**(k+1))**0.5) / (1 - beta1**(k+1))\n",
        "      else:\n",
        "          bias_correction = 1\n",
        "\n",
        "      dlr = d*lr*bias_correction\n",
        "\n",
        "      growth_rate = group['growth_rate']\n",
        "      decouple = group['decouple']\n",
        "      fsdp_in_use = group['fsdp_in_use']\n",
        "\n",
        "      d_numerator = group['d_numerator']\n",
        "      d_numerator *= beta3\n",
        "\n",
        "      for group in self.param_groups:\n",
        "          decay = group['weight_decay']\n",
        "          k = group['k']\n",
        "          eps = group['eps']\n",
        "          group_lr = group['lr']\n",
        "          d0 = group['d0']\n",
        "          safeguard_warmup = group['safeguard_warmup']\n",
        "\n",
        "          if group_lr not in [lr, 0.0]:\n",
        "              raise RuntimeError(f\"Setting different lr values in different parameter groups is only supported for values of 0\")\n",
        "\n",
        "          for p in group['params']:\n",
        "              if p.grad is None:\n",
        "                  continue\n",
        "              if hasattr(p, \"_fsdp_flattened\"):\n",
        "                  fsdp_in_use = True\n",
        "\n",
        "              grad = p.grad.data\n",
        "\n",
        "              # Apply weight decay (coupled variant)\n",
        "              if decay != 0 and not decouple:\n",
        "                  grad.add_(p.data, alpha=decay)\n",
        "\n",
        "              state = self.state[p]\n",
        "\n",
        "              # State initialization\n",
        "              if 'step' not in state:\n",
        "                  state['step'] = 0\n",
        "                  state['s'] = torch.zeros_like(p.data).detach()\n",
        "                  state['p0'] = p.detach().clone()\n",
        "                  # Exponential moving average of gradient values\n",
        "                  state['exp_avg'] = torch.zeros_like(p.data).detach()\n",
        "                  # Exponential moving average of squared gradient values\n",
        "                  state['exp_avg_sq'] = torch.zeros_like(p.data).detach()\n",
        "\n",
        "              exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "\n",
        "              s = state['s']\n",
        "              p0 = state['p0']\n",
        "\n",
        "              if group_lr > 0.0:\n",
        "                  # we use d / d0 instead of just d to avoid getting values that are too small\n",
        "                  d_numerator += (d / d0) * dlr * torch.dot(grad.flatten(), (p0.data - p.data).flatten()).item()\n",
        "\n",
        "                  # Adam EMA updates\n",
        "                  exp_avg.mul_(beta1).add_(grad, alpha=d * (1-beta1))\n",
        "                  exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=d * d * (1-beta2))\n",
        "\n",
        "                  if safeguard_warmup:\n",
        "                      s.mul_(beta3).add_(grad, alpha=((d / d0) * d))\n",
        "                  else:\n",
        "                      s.mul_(beta3).add_(grad, alpha=((d / d0) * dlr))\n",
        "                  d_denom += s.abs().sum().item()\n",
        "\n",
        "          ######\n",
        "\n",
        "      d_hat = d\n",
        "\n",
        "      # if we have not done any progres, return\n",
        "      # if we have any gradients available, will have d_denom > 0 (unless \\|g\\|=0)\n",
        "      if d_denom == 0:\n",
        "          return loss\n",
        "\n",
        "      if lr > 0.0:\n",
        "          if fsdp_in_use:\n",
        "              dist_tensor = torch.zeros(2).cuda()\n",
        "              dist_tensor[0] = d_numerator\n",
        "              dist_tensor[1] = d_denom\n",
        "              dist.all_reduce(dist_tensor, op=dist.ReduceOp.SUM)\n",
        "              global_d_numerator = dist_tensor[0]\n",
        "              global_d_denom = dist_tensor[1]\n",
        "          else:\n",
        "              global_d_numerator = d_numerator\n",
        "              global_d_denom = d_denom\n",
        "\n",
        "          d_hat = d_coef * global_d_numerator / global_d_denom\n",
        "          if d == group['d0']:\n",
        "              d = max(d, d_hat)\n",
        "          d_max = max(d_max, d_hat)\n",
        "          d = min(d_max, d * growth_rate)\n",
        "\n",
        "      for group in self.param_groups:\n",
        "          group['d_numerator'] = global_d_numerator\n",
        "          group['d_denom'] = global_d_denom\n",
        "          group['d'] = d\n",
        "          group['d_max'] = d_max\n",
        "          group['d_hat'] = d_hat\n",
        "\n",
        "          decay = group['weight_decay']\n",
        "          k = group['k']\n",
        "          eps = group['eps']\n",
        "\n",
        "          for p in group['params']:\n",
        "              if p.grad is None:\n",
        "                  continue\n",
        "              grad = p.grad.data\n",
        "\n",
        "              state = self.state[p]\n",
        "\n",
        "              exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "\n",
        "              state['step'] += 1\n",
        "\n",
        "              denom = exp_avg_sq.sqrt().add_(d * eps)\n",
        "\n",
        "              # Apply weight decay (decoupled variant)\n",
        "              if decay != 0 and decouple:\n",
        "                  p.data.add_(p.data, alpha=-decay * dlr)\n",
        "\n",
        "\n",
        "              ### Take step\n",
        "              p.data.addcdiv_(exp_avg, denom, value=-dlr)\n",
        "\n",
        "          group['k'] = k + 1\n",
        "\n",
        "      return loss"
      ],
      "metadata": {
        "id": "k7rjYHt1oedN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Ranger(Optimizer):\n",
        "\n",
        "  def __init__(self, params, lr=1e-3,                       # lr\n",
        "                alpha=0.5, k=6, N_sma_threshhold=5,           # Ranger options\n",
        "                betas=(.95, 0.999), eps=1e-5, weight_decay=0,  # Adam options\n",
        "                # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n",
        "                use_gc=True, gc_conv_only=False\n",
        "                ):\n",
        "\n",
        "      # parameter checks\n",
        "      if not 0.0 <= alpha <= 1.0:\n",
        "          raise ValueError(f'Invalid slow update rate: {alpha}')\n",
        "      if not 1 <= k:\n",
        "          raise ValueError(f'Invalid lookahead steps: {k}')\n",
        "      if not lr > 0:\n",
        "          raise ValueError(f'Invalid Learning Rate: {lr}')\n",
        "      if not eps > 0:\n",
        "          raise ValueError(f'Invalid eps: {eps}')\n",
        "\n",
        "      # parameter comments:\n",
        "      # beta1 (momentum) of .95 seems to work better than .90...\n",
        "      # N_sma_threshold of 5 seems better in testing than 4.\n",
        "      # In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n",
        "\n",
        "      # prep defaults and init torch.optim base\n",
        "      defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas,\n",
        "                      N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n",
        "      super().__init__(params, defaults)\n",
        "\n",
        "      # adjustable threshold\n",
        "      self.N_sma_threshhold = N_sma_threshhold\n",
        "\n",
        "      # look ahead params\n",
        "\n",
        "      self.alpha = alpha\n",
        "      self.k = k\n",
        "\n",
        "      # radam buffer for state\n",
        "      self.radam_buffer = [[None, None, None] for ind in range(10)]\n",
        "\n",
        "      # gc on or off\n",
        "      self.use_gc = use_gc\n",
        "\n",
        "      # level of gradient centralization\n",
        "      self.gc_gradient_threshold = 3 if gc_conv_only else 1\n",
        "\n",
        "  def __setstate__(self, state):\n",
        "      super(Ranger, self).__setstate__(state)\n",
        "\n",
        "  def step(self, closure=None):\n",
        "      loss = None\n",
        "      # Evaluate averages and grad, update param tensors\n",
        "      for group in self.param_groups:\n",
        "\n",
        "          for p in group['params']:\n",
        "              if p.grad is None:\n",
        "                  continue\n",
        "              grad = p.grad.data.float()\n",
        "\n",
        "              if grad.is_sparse:\n",
        "                  raise RuntimeError(\n",
        "                      'Ranger optimizer does not support sparse gradients')\n",
        "\n",
        "              p_data_fp32 = p.data.float()\n",
        "\n",
        "              state = self.state[p]  # get state dict for this param\n",
        "\n",
        "              if len(state) == 0:  # if first time to run...init dictionary with our desired entries\n",
        "                  # if self.first_run_check==0:\n",
        "                  # self.first_run_check=1\n",
        "                  #print(\"Initializing slow buffer...should not see this at load from saved model!\")\n",
        "                  state['step'] = 0\n",
        "                  state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                  state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "\n",
        "                  # look ahead weight storage now in state dict\n",
        "                  state['slow_buffer'] = torch.empty_like(p.data)\n",
        "                  state['slow_buffer'].copy_(p.data)\n",
        "\n",
        "              else:\n",
        "                  state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                  state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n",
        "                      p_data_fp32)\n",
        "\n",
        "              # begin computations\n",
        "              exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "              beta1, beta2 = group['betas']\n",
        "\n",
        "              # GC operation for Conv layers and FC layers\n",
        "              if grad.dim() > self.gc_gradient_threshold:\n",
        "                  grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n",
        "\n",
        "              state['step'] += 1\n",
        "\n",
        "              # compute variance mov avg\n",
        "              exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "              # compute mean moving avg\n",
        "              exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "              buffered = self.radam_buffer[int(state['step'] % 10)]\n",
        "\n",
        "              if state['step'] == buffered[0]:\n",
        "                  N_sma, step_size = buffered[1], buffered[2]\n",
        "              else:\n",
        "                  buffered[0] = state['step']\n",
        "                  beta2_t = beta2 ** state['step']\n",
        "                  N_sma_max = 2 / (1 - beta2) - 1\n",
        "                  N_sma = N_sma_max - 2 * \\\n",
        "                      state['step'] * beta2_t / (1 - beta2_t)\n",
        "                  buffered[1] = N_sma\n",
        "                  if N_sma > self.N_sma_threshhold:\n",
        "                      step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (\n",
        "                          N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                  else:\n",
        "                      step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                  buffered[2] = step_size\n",
        "\n",
        "              if group['weight_decay'] != 0:\n",
        "                  p_data_fp32.add_(-group['weight_decay']\n",
        "                                    * group['lr'], p_data_fp32)\n",
        "\n",
        "              # apply lr\n",
        "              if N_sma > self.N_sma_threshhold:\n",
        "                  denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                  p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size*\n",
        "                                        group['lr'])\n",
        "\n",
        "              else:\n",
        "                  p_data_fp32.add_(exp_avg, alpha=-step_size * group['lr'])\n",
        "\n",
        "              p.data.copy_(p_data_fp32)\n",
        "\n",
        "              # integrated look ahead...\n",
        "              # we do it at the param level instead of group level\n",
        "              if state['step'] % group['k'] == 0:\n",
        "                  # get access to slow param tensor\n",
        "                  slow_p = state['slow_buffer']\n",
        "                  # (fast weights - slow weights) * alpha\n",
        "                  slow_p.add_(p.data - slow_p, alpha=self.alpha)\n",
        "                  # copy interpolated weights to RAdam param tensor\n",
        "                  p.data.copy_(slow_p)\n",
        "\n",
        "      return loss"
      ],
      "metadata": {
        "id": "_KI24XUUoqoT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "ZM6UTicNroEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks\n",
        "https://arxiv.org/abs/1503.00075\n",
        "\"\"\"\n",
        "import itertools\n",
        "import time\n",
        "\n",
        "import dgl\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "CJJABzClqNx0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TreeLSTMCell(nn.Module):\n",
        "  def __init__(self, x_size, h_size):\n",
        "      super(TreeLSTMCell, self).__init__()\n",
        "      self.W_iou = nn.Linear(x_size, 3 * h_size, bias=False)\n",
        "      self.U_iou = nn.Linear(2 * h_size, 3 * h_size, bias=False)\n",
        "      self.b_iou = nn.Parameter(th.zeros(1, 3 * h_size))\n",
        "      self.U_f = nn.Linear(2 * h_size, 2 * h_size)\n",
        "\n",
        "  def message_func(self, edges):\n",
        "      return {\"h\": edges.src[\"h\"], \"c\": edges.src[\"c\"]}\n",
        "\n",
        "  def reduce_func(self, nodes):\n",
        "      h_cat = nodes.mailbox[\"h\"].view(nodes.mailbox[\"h\"].size(0), -1)\n",
        "      f = th.sigmoid(self.U_f(h_cat)).view(*nodes.mailbox[\"h\"].size())\n",
        "      c = th.sum(f * nodes.mailbox[\"c\"], 1)\n",
        "      return {\"iou\": self.U_iou(h_cat), \"c\": c}\n",
        "\n",
        "  def apply_node_func(self, nodes):\n",
        "      iou = nodes.data[\"iou\"] + self.b_iou\n",
        "      i, o, u = th.chunk(iou, 3, 1)\n",
        "      i, o, u = th.sigmoid(i), th.sigmoid(o), th.tanh(u)\n",
        "      c = i * u + nodes.data[\"c\"]\n",
        "      h = o * th.tanh(c)\n",
        "      return {\"h\": h, \"c\": c}\n",
        "\n",
        "\n",
        "class ChildSumTreeLSTMCell(nn.Module):\n",
        "  def __init__(self, x_size, h_size):\n",
        "      super(ChildSumTreeLSTMCell, self).__init__()\n",
        "      self.W_iou = nn.Linear(x_size, 3 * h_size, bias=False)\n",
        "      self.U_iou = nn.Linear(h_size, 3 * h_size, bias=False)\n",
        "      self.b_iou = nn.Parameter(th.zeros(1, 3 * h_size))\n",
        "      self.U_f = nn.Linear(h_size, h_size)\n",
        "\n",
        "  def message_func(self, edges):\n",
        "      return {\"h\": edges.src[\"h\"], \"c\": edges.src[\"c\"]}\n",
        "\n",
        "  def reduce_func(self, nodes):\n",
        "      h_tild = th.sum(nodes.mailbox[\"h\"], 1)\n",
        "      f = th.sigmoid(self.U_f(nodes.mailbox[\"h\"]))\n",
        "      c = th.sum(f * nodes.mailbox[\"c\"], 1)\n",
        "      return {\"iou\": self.U_iou(h_tild), \"c\": c}\n",
        "\n",
        "  def apply_node_func(self, nodes):\n",
        "      iou = nodes.data[\"iou\"] + self.b_iou\n",
        "      i, o, u = th.chunk(iou, 3, 1)\n",
        "      i, o, u = th.sigmoid(i), th.sigmoid(o), th.tanh(u)\n",
        "      c = i * u + nodes.data[\"c\"]\n",
        "      h = o * th.tanh(c)\n",
        "      return {\"h\": h, \"c\": c}\n",
        "\n",
        "\n",
        "class TreeLSTM(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_vocabs,\n",
        "      x_size,\n",
        "      h_size,\n",
        "      num_classes,\n",
        "      dropout,\n",
        "      cell_type=\"nary\",\n",
        "      pretrained_emb=None,\n",
        "  ):\n",
        "      super(TreeLSTM, self).__init__()\n",
        "      self.x_size = x_size\n",
        "      self.embedding = nn.Embedding(num_vocabs, x_size)\n",
        "      if pretrained_emb is not None:\n",
        "          print(\"Using glove\")\n",
        "          self.embedding.weight.data.copy_(pretrained_emb)\n",
        "          self.embedding.weight.requires_grad = True\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      self.linear = nn.Linear(h_size, num_classes)\n",
        "      cell = TreeLSTMCell if cell_type == \"nary\" else ChildSumTreeLSTMCell\n",
        "      self.cell = cell(x_size, h_size)\n",
        "\n",
        "  def forward(self, batch, g, h, c):\n",
        "      \"\"\"Compute tree-lstm prediction given a batch.\n",
        "      Parameters\n",
        "      ----------\n",
        "      batch : dgl.data.SSTBatch\n",
        "          The data batch.\n",
        "      g : dgl.DGLGraph\n",
        "          Tree for computation.\n",
        "      h : Tensor\n",
        "          Initial hidden state.\n",
        "      c : Tensor\n",
        "          Initial cell state.\n",
        "      Returns\n",
        "      -------\n",
        "      logits : Tensor\n",
        "          The prediction of each node.\n",
        "      \"\"\"\n",
        "      # feed embedding\n",
        "      embeds = self.embedding(batch.wordid * batch.mask)\n",
        "      g.ndata[\"iou\"] = self.cell.W_iou(\n",
        "          self.dropout(embeds)\n",
        "      ) * batch.mask.float().unsqueeze(-1)\n",
        "      g.ndata[\"h\"] = h\n",
        "      g.ndata[\"c\"] = c\n",
        "      # propagate\n",
        "      dgl.prop_nodes_topo(\n",
        "          g,\n",
        "          self.cell.message_func,\n",
        "          self.cell.reduce_func,\n",
        "          apply_node_func=self.cell.apply_node_func,\n",
        "      )\n",
        "      # compute logits\n",
        "      h = self.dropout(g.ndata.pop(\"h\"))\n",
        "      logits = self.linear(h)\n",
        "      return logits"
      ],
      "metadata": {
        "id": "ZxjjtQWQqFuw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "5amTWt9orqlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SSTBatch = collections.namedtuple(\n",
        "    \"SSTBatch\", [\"graph\", \"mask\", \"wordid\", \"label\"]\n",
        ")"
      ],
      "metadata": {
        "id": "UNmVkU9iovTy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batcher(device):\n",
        "  def batcher_dev(batch):\n",
        "      batch_trees = dgl.batch(batch)\n",
        "      return SSTBatch(\n",
        "          graph=batch_trees,\n",
        "          mask=batch_trees.ndata[\"mask\"].to(device),\n",
        "          wordid=batch_trees.ndata[\"x\"].to(device),\n",
        "          label=batch_trees.ndata[\"y\"].to(device),\n",
        "      )\n",
        "\n",
        "  return batcher_dev"
      ],
      "metadata": {
        "id": "u-wi5qUco0NB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\""
      ],
      "metadata": {
        "id": "4hobIHrspkOf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = SSTDataset()\n",
        "train_loader = DataLoader(\n",
        "    dataset=trainset,\n",
        "    batch_size=20,\n",
        "    collate_fn=batcher(device),\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "devset = SSTDataset(mode=\"dev\")\n",
        "dev_loader = DataLoader(\n",
        "    dataset=devset,\n",
        "    batch_size=100,\n",
        "    collate_fn=batcher(device),\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "testset = SSTDataset(mode=\"test\")\n",
        "test_loader = DataLoader(\n",
        "    dataset=testset,\n",
        "    batch_size=100,\n",
        "    collate_fn=batcher(device),\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        ")"
      ],
      "metadata": {
        "id": "EYtSCFoOo2KE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "dc0a1d994a2c45b880cb2166f45026d1",
            "3353940f1ba74b308a1bcc1b26ef1241",
            "0bd32cccc35a4bf08aa27ccc70e5eb64",
            "3f659fad49f148168d5152bf298026d7",
            "73fcce18779744c5bac1865528cd777c",
            "6c1c33f1df654575a38185328715b501",
            "c5d5029bac914ab2b63f72a44f3da7c3",
            "48b0d590c44e4d58af5c6bfa0cbce5ea",
            "4d09800c24d54291bd584aefce7d1d65",
            "a1cea837961143ee99970b5c4ec7bf29",
            "a02ccb9596284d1583687617e0d8ad2a"
          ]
        },
        "outputId": "41f6e9c4-efcf-4085-8874-65cf60def069"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading /root/.dgl/sst.zip from https://data.dgl.ai/dataset/sst.zip...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/root/.dgl/sst.zip:   0%|          | 0.00/930k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc0a1d994a2c45b880cb2166f45026d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting file to /root/.dgl/sst_c63ddc86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizers = [optim.SGD, optim.Adam, optim.RMSprop, optim.Adadelta, Ranger, Prodigy, NAG, ProxSG, SVRG]\n",
        "optimizer_names = ['SGD', 'Adam', 'RMSProp', 'Adadelta', 'Ranger', 'Prodigy', 'NAG', 'ProxSG', 'SVRG']"
      ],
      "metadata": {
        "id": "Q4YurCUTpNtP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dictionaries to store the results\n",
        "train_loss = {}\n",
        "test_accuracy = {}\n",
        "test_root_accuracy = {}\n",
        "convergence_rate = {}"
      ],
      "metadata": {
        "id": "3PKXPNfvprg3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over each optimizer\n",
        "for opt_fn, opt_name in zip(optimizers, optimizer_names):\n",
        "  best_dev_acc = 0\n",
        "  print(f\"\\nTraining with {opt_name} optimizer:\")\n",
        "\n",
        "  # Initialize your model and optimizer\n",
        "  model = TreeLSTM(\n",
        "      trainset.vocab_size,\n",
        "      300,\n",
        "      150,\n",
        "      trainset.num_classes,\n",
        "      0.5,\n",
        "      cell_type=\"childsum\" if \"store_true\" else \"nary\",\n",
        "      pretrained_emb=trainset.pretrained_emb,\n",
        "  ).to(device)\n",
        "\n",
        "  params_ex_emb = [\n",
        "      x\n",
        "      for x in list(model.parameters())\n",
        "      if x.requires_grad and x.size(0) != trainset.vocab_size\n",
        "  ]\n",
        "  params_emb = list(model.embedding.parameters())\n",
        "\n",
        "  for p in params_ex_emb:\n",
        "      if p.dim() > 1:\n",
        "          INIT.xavier_uniform_(p)\n",
        "\n",
        "  if opt_name == 'Prodigy' or opt_name == 'NAG':\n",
        "      optimizer = opt_fn(model.parameters(), lr=0.05)\n",
        "  elif opt_name == 'ProxSG':\n",
        "      optimizer = opt_fn(model.parameters(), lr=0.05, lambda_=0.0001)\n",
        "  else:\n",
        "      optimizer = opt_fn(\n",
        "          [\n",
        "              {\n",
        "                  \"params\": params_ex_emb,\n",
        "                  \"lr\": 0.05,\n",
        "                  \"weight_decay\": 1e-4,\n",
        "              },\n",
        "              {\"params\": params_emb, \"lr\": 0.05},\n",
        "          ]\n",
        "      )\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  for epoch in range(100):\n",
        "      model.train()\n",
        "      pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{100}')\n",
        "      for step, batch in enumerate(pbar):\n",
        "          g = batch.graph.to(device)\n",
        "          n = g.num_nodes()\n",
        "          h = th.zeros((n, 150)).to(device)\n",
        "          c = th.zeros((n, 150)).to(device)\n",
        "\n",
        "          logits = model(batch, g, h, c)\n",
        "          logp = F.log_softmax(logits, 1)\n",
        "          loss = F.nll_loss(logp, batch.label, reduction=\"sum\")\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          epoch_loss =+ loss.item()\n",
        "          pbar.set_postfix({'loss': epoch_loss / (step+1)})\n",
        "\n",
        "      # Append the average loss for this epoch\n",
        "      losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "      # eval on dev set\n",
        "      accs = []\n",
        "      root_accs = []\n",
        "      model.eval()\n",
        "      for step, batch in enumerate(dev_loader):\n",
        "          g = batch.graph.to(device)\n",
        "          n = g.num_nodes()\n",
        "          with th.no_grad():\n",
        "              h = th.zeros((n, 150)).to(device)\n",
        "              c = th.zeros((n, 150)).to(device)\n",
        "              logits = model(batch, g, h, c)\n",
        "\n",
        "          pred = th.argmax(logits, 1)\n",
        "          acc = th.sum(th.eq(batch.label, pred)).item()\n",
        "          accs.append([acc, len(batch.label)])\n",
        "          root_ids = [\n",
        "              i for i in range(g.num_nodes()) if g.out_degrees(i) == 0\n",
        "          ]\n",
        "          root_acc = np.sum(\n",
        "              batch.label.cpu().data.numpy()[root_ids]\n",
        "              == pred.cpu().data.numpy()[root_ids]\n",
        "          )\n",
        "          root_accs.append([root_acc, len(root_ids)])\n",
        "\n",
        "          # Calculate the loss\n",
        "          logp = F.log_softmax(logits, 1)\n",
        "          loss = F.nll_loss(logp, batch.label, reduction=\"sum\")\n",
        "          val_loss =+ loss.item()\n",
        "\n",
        "      dev_acc = (\n",
        "          1.0 * np.sum([x[0] for x in accs]) / np.sum([x[1] for x in accs])\n",
        "      )\n",
        "      dev_root_acc = (\n",
        "          1.0\n",
        "          * np.sum([x[0] for x in root_accs])\n",
        "          / np.sum([x[1] for x in root_accs])\n",
        "      )\n",
        "\n",
        "      if dev_root_acc > best_dev_acc:\n",
        "          best_dev_acc = dev_root_acc\n",
        "          th.save(model.state_dict(), \"best_{}.pkl\".format(opt_name))\n",
        "\n",
        "      print(\"Validation loss: \", val_loss / len(dev_loader))\n",
        "      print(\"Validation accuracy: \", dev_acc)\n",
        "      print(\"Validation root accuracy: \", dev_root_acc)\n",
        "\n",
        "      # lr decay\n",
        "      for param_group in optimizer.param_groups:\n",
        "          param_group[\"lr\"] = max(1e-5, param_group[\"lr\"] * 0.99)  # 10\n",
        "          #print(param_group[\"lr\"])\n",
        "\n",
        "  # test\n",
        "  model.load_state_dict(th.load(\"best_{}.pkl\".format(opt_name)))\n",
        "  model.eval()\n",
        "  accs = []\n",
        "  root_accs = []\n",
        "  test_loss = 0\n",
        "  model.eval()\n",
        "  pbar = tqdm(test_loader, desc=f'Epoch {epoch+1}/{100}')\n",
        "  for step, batch in enumerate(pbar):\n",
        "      g = batch.graph.to(device)\n",
        "      n = g.num_nodes()\n",
        "      with th.no_grad():\n",
        "          h = th.zeros((n, 150)).to(device)\n",
        "          c = th.zeros((n, 150)).to(device)\n",
        "          logits = model(batch, g, h, c)\n",
        "\n",
        "      pred = th.argmax(logits, 1)\n",
        "      acc = th.sum(th.eq(batch.label, pred)).item()\n",
        "      accs.append([acc, len(batch.label)])\n",
        "      root_ids = [i for i in range(g.num_nodes()) if g.out_degrees(i) == 0]\n",
        "      root_acc = np.sum(\n",
        "          batch.label.cpu().data.numpy()[root_ids]\n",
        "          == pred.cpu().data.numpy()[root_ids]\n",
        "      )\n",
        "      root_accs.append([root_acc, len(root_ids)])\n",
        "\n",
        "      # Calculate the loss\n",
        "      logp = F.log_softmax(logits, 1)\n",
        "      loss = F.nll_loss(logp, batch.label, reduction=\"sum\")\n",
        "      test_loss =+ loss.item()\n",
        "      pbar.set_postfix({'loss': test_loss / (step+1)})\n",
        "\n",
        "  test_acc = 1.0 * np.sum([x[0] for x in accs]) / np.sum([x[1] for x in accs])\n",
        "  test_root_acc = (\n",
        "      1.0\n",
        "      * np.sum([x[0] for x in root_accs])\n",
        "      / np.sum([x[1] for x in root_accs])\n",
        "  )\n",
        "  print(\"Testing loss: \", test_loss / len(train_loader))\n",
        "  print(\"Testing accuracy: \", test_acc)\n",
        "  print(\"Testing root accuracy: \", test_root_acc)\n",
        "\n",
        "  conv_rate = [losses[i] - losses[i-1] for i in range(1, len(losses))]\n",
        "\n",
        "  # Store the results in the dictionaries\n",
        "  train_loss[opt_name] = losses\n",
        "  test_accuracy[opt_name] = test_acc*100\n",
        "  test_root_accuracy[opt_name] = test_root_acc*100\n",
        "  convergence_rate[opt_name] = conv_rate"
      ],
      "metadata": {
        "id": "1ySwfNxmpuwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(4, 1, figsize=(10, 20))\n",
        "\n",
        "# Plot losses\n",
        "for opt_name in optimizer_names:\n",
        "    losses = train_loss[opt_name]\n",
        "    axs[0].plot(losses, label=f'{opt_name}')\n",
        "axs[0].set_title('Training Losses')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].legend()\n",
        "\n",
        "# Plot test accuracies\n",
        "for opt_name in optimizer_names:\n",
        "    accuracy = test_accuracy[opt_name]\n",
        "    bar = axs[1].bar(opt_name, accuracy, label=f'{opt_name}')\n",
        "    axs[1].text(bar[0].get_x() + bar[0].get_width() / 2, bar[0].get_height(), f'{accuracy:.2f}%', ha='center', va='bottom')\n",
        "axs[1].set_title('Test Accuracy')\n",
        "axs[1].set_xlabel('Optimizer')\n",
        "axs[1].set_ylabel('Accuracy (%)')\n",
        "axs[1].legend()\n",
        "\n",
        "# Plot test root accuracies\n",
        "for opt_name in optimizer_names:\n",
        "    root_accuracy = test_root_accuracy[opt_name]\n",
        "    bar = axs[2].bar(opt_name, root_accuracy, label=f'{opt_name}')\n",
        "    axs[2].text(bar[0].get_x() + bar[0].get_width() / 2, bar[0].get_height(), f'{root_accuracy:.2f}%', ha='center', va='bottom')\n",
        "axs[2].set_title('Test Root Accuracy')\n",
        "axs[2].set_xlabel('Optimizer')\n",
        "axs[2].set_ylabel('Root Accuracy (%)')\n",
        "axs[2].legend()\n",
        "\n",
        "# Plot convergence rates\n",
        "for opt_name in optimizer_names:\n",
        "    losses = train_loss[opt_name]\n",
        "    convergence_rates = [losses[i] - losses[i-1] for i in range(1, len(losses))]\n",
        "    axs[3].plot(range(1, len(losses)), convergence_rates, label=f'{opt_name}')\n",
        "axs[3].set_title('Convergence Rate')\n",
        "axs[3].set_xlabel('Epoch')\n",
        "axs[3].set_ylabel('Convergence Rate')\n",
        "axs[3].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0a5yB4tAqBLY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}